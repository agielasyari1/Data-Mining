{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KNN or K-Nearest Neighbor Theory \u200b Algoritma K-Nearest Neighbor (KNN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data . Kelebihan dan Kekurangan KNN (K-Nearest Neighbor) Kelebihan KNN (K-Nearest Neighbor) Sangat nonlinear. Mudah dipahami dan diimplementasikan. Kekurangan KNN (K-Nearest Neighbor) Perlu menunjukkan parameter K (jumlah tetangga terdekat). Tidak menangani nilai hilang ( missing value ) secara implisit. Sensitif terhadap data pencilan ( outlier ). Rentan terhadap variabel yang non-informatif. Rentan terhadap dimensionalitas yang tinggi. Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari setiap sampel uji pada keseluruhan sampel latih. Study Kasus Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Social_Network_Ads.csv. Step 1, Import package dengan menggunakan python 3.7 library yang diperlukan : #imporing libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd import time Step 2, Feature scalling Wikipedia- \u201cSince the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\u201d \u200b Jadi karena klasifikasi KNN dihitung berdasarkan jarak Euclidean antar dua titik. Jika salah satu variabel memiliki jarak yang dengan rentang yang jauh(nilai yg jauh lebih tinggi),maka nilai tersebut harus disesuaikan. Jadi jarak antara semua variabel harus dinormalisasikan agar jarak akhir yang didapatkan proposional. #feature scaling class FeatureScaling: def __init__(self,X,y): self.X=X.copy() if y.ndim==1: y=np.reshape(y,(y.shape[0],1)) self.y=y.copy() self.minMax_X={} self.minMax_y={} def fit_transform_X(self): num_of_features=self.X.shape[1] for i in range(num_of_features): feature=self.X[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_X[i]=np.array([Mean,Min,Max]) self.X[:,i]=feature return self.X.copy() def fit_transform_Y(self): num_of_features=self.y.shape[1] for i in range(num_of_features): feature=self.y[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_y[i]=np.array([Mean,Min,Max]) self.y[:,i]=feature return np.reshape(self.y,self.y.shape[0]) def inverse_transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_X[i][1] Max=self.minMax_X[i][2] feature=feature*(Max-Min)+Mean X_transformed[:,i]=feature return X_transformed def inverse_transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=feature*(Max-Min)+Mean y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) X_transformed[:,i]=feature return X_transformed def transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def returnX(self): return self.X def returnY(self): return self.y Step 3 Train \u200b Saya menerapkan kelas KNN dengan fungsi standar 'cocok' untuk pelatihan dan 'prediksi' untuk memprediksi data uji. KNN menggunakan lazy algoritm yang berarti semua perhitungan ditangguhkan hingga prediksi. Dalam metode fit, saya hanya menetapkan data pelatihan ke variabel kelas - xtrain dan ytrain. Tidak diperlukan perhitungan. \u200b Saat saya mengulangi setiap baris pelatihan untuk mendapatkan skor kesamaan, saya menggunakan document_similarity fungsi kustom yang menerima dua teks dan mengembalikan skor kesamaan di antara mereka (0 & 1). Skor kesamaan yang lebih tinggi menunjukkan lebih banyak kesamaan di antara mereka. import numpy as np class KNN: def __init__(self,X_train,Y_train,K): self.X_train=X_train self.Y_train=Y_train self.K=K def predict(self,X): y_pred=np.array([]) for each in X: ed=np.sum((each-self.X_train)**2,axis=1) y_ed=np.concatenate((self.Y_train.reshape(self.Y_train.shape[0],1),ed.reshape(ed.shape[0],1)),axis=1) y_ed=y_ed[y_ed[:,1].argsort()] K_neighbours=y_ed[0:self.K] (values,counts) = np.unique(K_neighbours[:,0].astype(int),return_counts=True) y_pred=np.append(y_pred,values[np.argmax(counts)]) return y_pred Step 4 , Read data #reading dataset Data=pd.read_csv('Social_Network_Ads.csv') print(Data.head(10)) Data.describe() Output User ID Gender Age EstimatedSalary Purchased 0 15624510 Male 19 19000 0 1 15810944 Male 35 20000 0 2 15668575 Female 26 43000 0 3 15603246 Female 27 57000 0 4 15804002 Male 19 76000 0 5 15728773 Male 27 58000 0 6 15598044 Female 27 84000 0 7 15694829 Female 32 150000 1 8 15600575 Male 25 33000 0 9 15727311 Female 35 65000 0 Step 5, Traning & Testing saya menggambil sampel untuk ditraining dari keseluruhan data yaitu 75% dan untuk ditesting 25% #training and testing set size train_size=int(0.75*Data.shape[0]) test_size=int(0.25*Data.shape[0]) print(\"Training set size : \"+ str(train_size)) print(\"Testing set size : \"+str(test_size)) Training set size : 300 Testing set size : 100 #Getting features from dataset Data=Data.sample(frac=1) X=Data.iloc[:,[2, 3]].values y=Data.iloc[:,4].values X=X.astype(float) #feature scaling fs=FeatureScaling(X,y) X=fs.fit_transform_X() #training set split X_train=X[0:train_size,:] Y_train=y[0:train_size] #testing set split X_test=X[train_size:,:] Y_test=y[train_size:] l=time.time() knn=KNN(X_train,Y_train,5) y_pred=knn.predict(X_test) r=time.time() KNN_learn_time=(r-l) print(r-l) 0.017045021057128906 Step 6, Getting Matrix #getting the confusion matrix tp=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==0 and y_pred[i]==0]) tn=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==0 and y_pred[i]==1]) fp=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==1 and y_pred[i]==0]) fn=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==1 and y_pred[i]==1]) confusion_matrix=np.array([[tp,tn],[fp,fn]]) print(confusion_matrix) [[54 8] [ 6 32]] Step 7, Getting time Sebagai perbandingan waktu saja #Same algorithm using sklearn KNN just for comparsion purpose from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) l=time.time() classifier.fit(X_train, Y_train) y_pred_sklearn = classifier.predict(X_test) r=time.time() sklearn_time=(r-l) print(sklearn_time) 0.0020058155059814453 print(\"But sklearn time is faster than our implementation by: \"+str(KNN_learn_time/sklearn_time)+\" times\") But sklearn time is faster than our implementation by: 8.497801022227504 times # Making the Confusion Matrix from sklearn.metrics import confusion_matrix cm = confusion_matrix(Y_test, y_pred_sklearn) print(cm) [[54 8] [ 6 32]] Step 8, Implementasi matplotlib # Visualising the Training set results for our implementation l=time.time() from matplotlib.colors import ListedColormap X_set, y_set = X_train, Y_train X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Training set) using our implementation') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 25.802637815475464 seconds # Visualising the Test set results for our implementation l=time.time() X_set, y_set = X_test, Y_test X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Test set) using our implementation') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 22.427660942077637 seconds Skalearn # Visualising the Training set results for sklearn class l=time.time() X_set, y_set = X_train, Y_train X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Training set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 0.48911190032958984 seconds # Visualising the Test set results for sklearn class l=time.time() X_set, y_set = X_test, Y_test X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Test set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 0.47005295753479004 seconds Conclusion Algoritma KNN sangat intuitif dan mudah dimengerti, Waktu pengujian bisa sangat lama, karena algoritme melingkar di seluruh dataset training dan menghitung jarak (perhitungan jarak dapat menjadi lambat, berdasarkan pada jenis matrik jarak dan berdasarkan pada jenis dataset), Kumpulan data harus numerik atau matrik jarak harus ada untuk menghitung jarak antar titik, Tidak terlalu baik untuk digunakan pada data yang tidak seimbang References Advernesia. (2018, Mei 28). Pengertian dan Cara Kerja Algoritma K-Nearest Neighbors (KNN) . Retrieved from https://www.advernesia.com/blog/data-science/pengertian-dan-cara-kerja-algoritma-k-nearest-neighbours-knn/ Ismail, A. M. (2018, Agustus 17). Cara Kerja Algoritma k-Nearest Neighbor (k-NN) . Retrieved from https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e Solahudin, Y. (2017, Juli 1). Konsep Package dan Module di Python . Retrieved from https://medium.com/@yanwarsolah/konsep-package-dan-module-di-python-fe3e89e80d40 Zakka, K. (2016, Juli 13). Panduan Lengkap untuk K-Nearest-Neighbors dengan Aplikasi dalam Python dan R . Retrieved from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/","title":"KNN or K-Nearest Neighbor"},{"location":"#knn-or-k-nearest-neighbor","text":"","title":"KNN or K-Nearest Neighbor"},{"location":"#theory","text":"\u200b Algoritma K-Nearest Neighbor (KNN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data .","title":"Theory"},{"location":"#kelebihan-dan-kekurangan-knn-k-nearest-neighbor","text":"Kelebihan KNN (K-Nearest Neighbor) Sangat nonlinear. Mudah dipahami dan diimplementasikan. Kekurangan KNN (K-Nearest Neighbor) Perlu menunjukkan parameter K (jumlah tetangga terdekat). Tidak menangani nilai hilang ( missing value ) secara implisit. Sensitif terhadap data pencilan ( outlier ). Rentan terhadap variabel yang non-informatif. Rentan terhadap dimensionalitas yang tinggi. Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari setiap sampel uji pada keseluruhan sampel latih.","title":"Kelebihan dan Kekurangan KNN (K-Nearest Neighbor)"},{"location":"#study-kasus","text":"Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Social_Network_Ads.csv.","title":"Study Kasus"},{"location":"#step-1-import-package","text":"dengan menggunakan python 3.7 library yang diperlukan : #imporing libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd import time","title":"Step 1, Import package"},{"location":"#step-2-feature-scalling","text":"Wikipedia- \u201cSince the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\u201d \u200b Jadi karena klasifikasi KNN dihitung berdasarkan jarak Euclidean antar dua titik. Jika salah satu variabel memiliki jarak yang dengan rentang yang jauh(nilai yg jauh lebih tinggi),maka nilai tersebut harus disesuaikan. Jadi jarak antara semua variabel harus dinormalisasikan agar jarak akhir yang didapatkan proposional. #feature scaling class FeatureScaling: def __init__(self,X,y): self.X=X.copy() if y.ndim==1: y=np.reshape(y,(y.shape[0],1)) self.y=y.copy() self.minMax_X={} self.minMax_y={} def fit_transform_X(self): num_of_features=self.X.shape[1] for i in range(num_of_features): feature=self.X[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_X[i]=np.array([Mean,Min,Max]) self.X[:,i]=feature return self.X.copy() def fit_transform_Y(self): num_of_features=self.y.shape[1] for i in range(num_of_features): feature=self.y[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_y[i]=np.array([Mean,Min,Max]) self.y[:,i]=feature return np.reshape(self.y,self.y.shape[0]) def inverse_transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_X[i][1] Max=self.minMax_X[i][2] feature=feature*(Max-Min)+Mean X_transformed[:,i]=feature return X_transformed def inverse_transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=feature*(Max-Min)+Mean y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) X_transformed[:,i]=feature return X_transformed def transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def returnX(self): return self.X def returnY(self): return self.y","title":"Step 2, Feature scalling"},{"location":"#step-3-train","text":"\u200b Saya menerapkan kelas KNN dengan fungsi standar 'cocok' untuk pelatihan dan 'prediksi' untuk memprediksi data uji. KNN menggunakan lazy algoritm yang berarti semua perhitungan ditangguhkan hingga prediksi. Dalam metode fit, saya hanya menetapkan data pelatihan ke variabel kelas - xtrain dan ytrain. Tidak diperlukan perhitungan. \u200b Saat saya mengulangi setiap baris pelatihan untuk mendapatkan skor kesamaan, saya menggunakan document_similarity fungsi kustom yang menerima dua teks dan mengembalikan skor kesamaan di antara mereka (0 & 1). Skor kesamaan yang lebih tinggi menunjukkan lebih banyak kesamaan di antara mereka. import numpy as np class KNN: def __init__(self,X_train,Y_train,K): self.X_train=X_train self.Y_train=Y_train self.K=K def predict(self,X): y_pred=np.array([]) for each in X: ed=np.sum((each-self.X_train)**2,axis=1) y_ed=np.concatenate((self.Y_train.reshape(self.Y_train.shape[0],1),ed.reshape(ed.shape[0],1)),axis=1) y_ed=y_ed[y_ed[:,1].argsort()] K_neighbours=y_ed[0:self.K] (values,counts) = np.unique(K_neighbours[:,0].astype(int),return_counts=True) y_pred=np.append(y_pred,values[np.argmax(counts)]) return y_pred","title":"Step 3 Train"},{"location":"#step-4-read-data","text":"#reading dataset Data=pd.read_csv('Social_Network_Ads.csv') print(Data.head(10)) Data.describe()","title":"Step 4 , Read data"},{"location":"#output","text":"User ID Gender Age EstimatedSalary Purchased 0 15624510 Male 19 19000 0 1 15810944 Male 35 20000 0 2 15668575 Female 26 43000 0 3 15603246 Female 27 57000 0 4 15804002 Male 19 76000 0 5 15728773 Male 27 58000 0 6 15598044 Female 27 84000 0 7 15694829 Female 32 150000 1 8 15600575 Male 25 33000 0 9 15727311 Female 35 65000 0","title":"Output"},{"location":"#step-5-traning-testing","text":"saya menggambil sampel untuk ditraining dari keseluruhan data yaitu 75% dan untuk ditesting 25% #training and testing set size train_size=int(0.75*Data.shape[0]) test_size=int(0.25*Data.shape[0]) print(\"Training set size : \"+ str(train_size)) print(\"Testing set size : \"+str(test_size)) Training set size : 300 Testing set size : 100 #Getting features from dataset Data=Data.sample(frac=1) X=Data.iloc[:,[2, 3]].values y=Data.iloc[:,4].values X=X.astype(float) #feature scaling fs=FeatureScaling(X,y) X=fs.fit_transform_X() #training set split X_train=X[0:train_size,:] Y_train=y[0:train_size] #testing set split X_test=X[train_size:,:] Y_test=y[train_size:] l=time.time() knn=KNN(X_train,Y_train,5) y_pred=knn.predict(X_test) r=time.time() KNN_learn_time=(r-l) print(r-l) 0.017045021057128906","title":"Step 5, Traning &amp; Testing"},{"location":"#step-6-getting-matrix","text":"#getting the confusion matrix tp=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==0 and y_pred[i]==0]) tn=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==0 and y_pred[i]==1]) fp=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==1 and y_pred[i]==0]) fn=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==1 and y_pred[i]==1]) confusion_matrix=np.array([[tp,tn],[fp,fn]]) print(confusion_matrix) [[54 8] [ 6 32]]","title":"Step 6, Getting Matrix"},{"location":"#step-7-getting-time","text":"Sebagai perbandingan waktu saja #Same algorithm using sklearn KNN just for comparsion purpose from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) l=time.time() classifier.fit(X_train, Y_train) y_pred_sklearn = classifier.predict(X_test) r=time.time() sklearn_time=(r-l) print(sklearn_time) 0.0020058155059814453 print(\"But sklearn time is faster than our implementation by: \"+str(KNN_learn_time/sklearn_time)+\" times\") But sklearn time is faster than our implementation by: 8.497801022227504 times # Making the Confusion Matrix from sklearn.metrics import confusion_matrix cm = confusion_matrix(Y_test, y_pred_sklearn) print(cm) [[54 8] [ 6 32]]","title":"Step 7, Getting time"},{"location":"#step-8-implementasi","text":"","title":"Step 8, Implementasi"},{"location":"#matplotlib","text":"# Visualising the Training set results for our implementation l=time.time() from matplotlib.colors import ListedColormap X_set, y_set = X_train, Y_train X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Training set) using our implementation') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 25.802637815475464 seconds # Visualising the Test set results for our implementation l=time.time() X_set, y_set = X_test, Y_test X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Test set) using our implementation') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 22.427660942077637 seconds","title":"matplotlib"},{"location":"#skalearn","text":"# Visualising the Training set results for sklearn class l=time.time() X_set, y_set = X_train, Y_train X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Training set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 0.48911190032958984 seconds # Visualising the Test set results for sklearn class l=time.time() X_set, y_set = X_test, Y_test X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Test set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 0.47005295753479004 seconds","title":"Skalearn"},{"location":"#conclusion","text":"Algoritma KNN sangat intuitif dan mudah dimengerti, Waktu pengujian bisa sangat lama, karena algoritme melingkar di seluruh dataset training dan menghitung jarak (perhitungan jarak dapat menjadi lambat, berdasarkan pada jenis matrik jarak dan berdasarkan pada jenis dataset), Kumpulan data harus numerik atau matrik jarak harus ada untuk menghitung jarak antar titik, Tidak terlalu baik untuk digunakan pada data yang tidak seimbang","title":"Conclusion"},{"location":"#references","text":"Advernesia. (2018, Mei 28). Pengertian dan Cara Kerja Algoritma K-Nearest Neighbors (KNN) . Retrieved from https://www.advernesia.com/blog/data-science/pengertian-dan-cara-kerja-algoritma-k-nearest-neighbours-knn/ Ismail, A. M. (2018, Agustus 17). Cara Kerja Algoritma k-Nearest Neighbor (k-NN) . Retrieved from https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e Solahudin, Y. (2017, Juli 1). Konsep Package dan Module di Python . Retrieved from https://medium.com/@yanwarsolah/konsep-package-dan-module-di-python-fe3e89e80d40 Zakka, K. (2016, Juli 13). Panduan Lengkap untuk K-Nearest-Neighbors dengan Aplikasi dalam Python dan R . Retrieved from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/","title":"References"},{"location":"Decison Trees/","text":"Decision Tree Classifier Theory \u200b Pohon keputusan adalah struktur seperti bagan alur di mana setiap simpul internal mewakili \"pengujian\" pada atribut , masing-masing cabang mewakili hasil pengujian, dan setiap simpul daun mewakili label kelas (keputusan diambil setelah menghitung semua atribut). Jalur dari root ke daun mewakili aturan klasifikasi. Dalam analisis keputusan, pohon keputusan dan diagram pengaruh yang terkait erat digunakan sebagai alat pendukung keputusan visual dan analitis, \u200b Pohon keputusan biasanya digunakan dalam riset operasi dan manajemen operasi. Jika, dalam praktiknya, keputusan harus diambil tanpa penarikan kembali di bawah pengetahuan yang tidak lengkap, pohon keputusan harus diparalelkan dengan model probabilitas sebagai model pilihan terbaik atau algoritma model seleksi. Penggunaan lain dari pohon keputusan adalah sebagai alat deskriptif untuk menghitung probabilitas bersyarat. \u200b (Dahulu dibuat Secara Manual) Tingkat kesalahan klasifikasi hanyalah sebagian kecil dari pengamatan pelatihan di suatu wilayah yang bukan milik kelas yang paling umum. $$ E=1-\\max \\left(\\hat{p}_{m k}\\right) $$ \u200b (Formula tingkat kesalahan klasifikasi) Namun, ini tidak cukup sensitif untuk penanaman pohon. Dalam praktiknya, dua metode lain digunakan. $$ G=\\sum_{k=1}^{K} \\hat{p} {m k}\\left(1-\\hat{p} {m k}\\right) $$ \u200b (Gini Index) Ini adalah ukuran varian total di semua kelas. Seperti yang Anda lihat, indeks Gini akan kecil jika proporsinya mendekati 0 atau 1, jadi ini adalah ukuran yang baik untuk pengujian. Hal yang serupa diterapkan pada metode lain yang disebut cross-entropy. $$ D=-\\sum_{k=1}^{K} \\hat{p} {m k} \\log \\left(\\hat{p} {m k}\\right) $$ \u200b (Entropy) Bagging, random forests and boosting Bagging \u200b Sebelumnya dapat menghitung standar deviasi dari jumlah yang diinginkan. Untuk pohon keputusan, variansnya sangat tinggi. Oleh karena itu, dengan agregasi bootstrap atau bagging , kita dapat mengurangi varians dan meningkatkan kinerja pohon keputusan. \u200b Bagging melibatkan berulang kali mengambil sampel dari dataset. Ini menghasilkan B set pelatihan bootstrap yang berbeda. Kemudian, melatih pada semua set pelatihan bootstrap untuk mendapatkan prediksi untuk setiap set, dan kemudian rata-rata prediksi. $$ \\hat{f} {b a g}(x)=\\frac{1}{B} \\sum {b=1}^{B} \\hat{f}^{* b}(x) $$ \u200b Menerapkan ini ke pohon keputusan, itu berarti bahwa kita dapat membangun sejumlah besar pohon yang akan memiliki varian tinggi dan bias rendah. Kemudian, kita dapat meratakan prediksi mereka untuk mengurangi varians untuk meningkatkan kinerja pohon keputusan. Random forests \u200b Random Forests memberikan peningkatan dari pada bagging , dengan cara mengecek mulai bagian kecil yang ada pada pohon-pohon tersebut. Seperti dalam bagging, banyak pohon keputusan dibangun. Namun, pada setiap split, sampel acak dari prediktor m dipilih dari semua prediktor p. Pemisahan ini diizinkan untuk menggunakan hanya satu dari prediktor m, Dengan kata lain, pada setiap pemisahan, algoritma tidak diperbolehkan untuk mempertimbangkan mayoritas prediktor yang tersedia! $$ m=\\sqrt{p} $$ ( Kesalahan klasifikasi sebagai fungsi dari jumlah pohon. Setiap baris mewakili jumlah prediktor yang tersedia di setiap pemisahan ) Boosting \u200b Tidak jauh beda dengan Bagging dan Random forest , namun Bossting mengkoreksi setiap pohon Ini berarti bahwa algoritma belajar dengan lambat. Setiap pohon dengan residu dari model dari pada variabel target. Oleh karena itu, setiap pohon kecil dan perlahan akan meningkatkan prediksi di daerah di mana ia tidak bekerja dengan baik ( Kesalahan klasifikasi sebagai fungsi dari jumlah pohon. Setiap baris mewakili kedalaman interaksi yang berbeda.) Study Kasus \u200b Banyak dataset tentang kanker payudara mengandung informasi tentang tumor. Namun, saya beruntung menemukan dataset yang berisi informasi tes darah rutin pasien dengan dan tanpa kanker payudara. Berpotensi, jika kita dapat memprediksi secara akurat jika seorang pasien menderita kanker, pasien tersebut dapat menerima perawatan yang sangat dini, bahkan sebelum tumor terlihat! \u200b Saya memperoleh dataset di sini . Pertama, yang perlu diperhatikan bahwa dataset sangat kecil, dengan hanya 116 instance. Ini menimbulkan beberapa tantangan, karena pohon keputusan mungkin sesuai dengan data, atau model prediksi yang saya lakukan mungkin bukan yang terbaik, karena kurangnya pengamatan lain. Namun, itu adalah bukti konsep yang baik yang mungkin menunjukkan potensi nyata untuk memprediksi kanker payudara dari tes darah sederhana. Step 1, Import package dengan menggunakan python 3.7 library yang diperlukan : import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns Step 2, Read Data DATAPATH = 'data/dataR2.csv' data = pd.read_csv(DATAPATH) data.head() Step 3, Classification Mengklasifikasi data. x = data['Classification'] ax = sns.countplot(x=x, data=data) \u200b (Seperti yang kita lihat, ada jumlah pasien dan kontrol yang hampir sama.) Step 4, Ploting Sekarang, akan menarik untuk melihat distribusi dan kepadatan setiap fitur untuk orang sehat dan pasien. Untuk melakukannya, violin plots sangat ideal. Ini menunjukkan kepadatan dan distribusi fitur dalam satu plot. y = data.columns[:-1] x = data.columns[-1] def violin_plots(x, y, data): for i, col in enumerate(y): plt.figure(i) sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.violinplot(x=x, y=col, data=data) violin_plots(x, y, data) Step 5, Modelling Pertama, kita perlu menyandikan kelas ke 0 dan 1: from sklearn.preprocessing import LabelEncoder le = LabelEncoder() data['Classification'] = le.fit_transform(data['Classification']) data.head() Sekarang, 0 mewakili kontrol yang sehat, dan 1 mewakili pasien. Kemudian, membagi dataset menjadi set pelatihan dan tes: from sklearn.model_selection import train_test_split y = data['Classification'].values.reshape(-1, 1) X = data.drop('Classification', 1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) Step 6, Matrix Perlu menentukan metrik kesalahan yang sesuai. Karena ini adalah masalah klasifikasi, import itertools def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(\"Normalized confusion matrix\") else: print('Confusion matrix, without normalization') print(cm) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes) plt.yticks(tick_marks, classes) fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\") plt.ylabel('True label') plt.xlabel('Predicted label') plt.tight_layout() Step 7, Implementasi Decision tree from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import confusion_matrix clf = DecisionTreeClassifier() clf.fit(X_train, y_train) y_pred = clf.predict(X_test) decision_tree_cm = confusion_matrix(y_test, y_pred) plot_confusion_matrix(decision_tree_cm, [0, 1]) plt.show() Output Confusion matrix, without normalization [[6 3] [0 3]] Bagging from sklearn.ensemble import BaggingClassifier bagging_clf = BaggingClassifier() bagging_clf.fit(X_train, y_train.ravel()) y_pred_bag = bagging_clf.predict(X_test) bag_cm = confusion_matrix(y_test, y_pred_bag) plot_confusion_matrix(bag_cm, [0, 1]) plt.show() Output Confusion matrix, without normalization [[9 0] [0 3]] Random forest Di sini, untuk klasifikasi Random forest , Menggunakan 100 dataset: from sklearn.ensemble import RandomForestClassifier random_clf = RandomForestClassifier(100) random_clf.fit(X_train, y_train.ravel()) y_pred_random = random_clf.predict(X_test) random_cm = confusion_matrix(y_test, y_pred_random) plot_confusion_matrix(random_cm, [0, 1]) plt.show() Output Confusion matrix, without normalization [[9 0] [1 2]] Boosting from sklearn.ensemble import GradientBoostingClassifier boost_clf = GradientBoostingClassifier() boost_clf.fit(X_train, y_train.ravel()) y_pred_boost = boost_clf.predict(X_test) boost_cm = confusion_matrix(y_test, y_pred_boost) plot_confusion_matrix(boost_cm, [0, 1]) plt.show() Output Confusion matrix, without normalization [[8 1] [0 3]] Conclusion \u200b Kita telah melihat bagaimana menerapkan pohon keputusan dan bagaimana meningkatkan kinerjanya dengan bosoting , bagging , dan random forest . Tampaknya bagging memberikan hasil terbaik, karena mengklasifikasikan semua instance dengan benar. Namun, ada yang harus ingat bahwa dataset itu sangat kecil. Meskipun itu menunjukkan bahwa kita berpotensi dapat memprediksi kanker payudara dari tes darah, algoritmanya tidak mungkin bekerja dengan baik pada data yang tidak terlihat, karena tidak ada cukup data. References Machine Learning Course ( https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd ) Dicision trees in maschine learning ( https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052?source=search_post---------0 ) Deep math machine learning ( https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1 ) Decision trees for machine learning and data science ( https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956 )","title":"Decision Tree Classifier"},{"location":"Decison Trees/#decision-tree-classifier","text":"","title":"Decision Tree Classifier"},{"location":"Decison Trees/#theory","text":"\u200b Pohon keputusan adalah struktur seperti bagan alur di mana setiap simpul internal mewakili \"pengujian\" pada atribut , masing-masing cabang mewakili hasil pengujian, dan setiap simpul daun mewakili label kelas (keputusan diambil setelah menghitung semua atribut). Jalur dari root ke daun mewakili aturan klasifikasi. Dalam analisis keputusan, pohon keputusan dan diagram pengaruh yang terkait erat digunakan sebagai alat pendukung keputusan visual dan analitis, \u200b Pohon keputusan biasanya digunakan dalam riset operasi dan manajemen operasi. Jika, dalam praktiknya, keputusan harus diambil tanpa penarikan kembali di bawah pengetahuan yang tidak lengkap, pohon keputusan harus diparalelkan dengan model probabilitas sebagai model pilihan terbaik atau algoritma model seleksi. Penggunaan lain dari pohon keputusan adalah sebagai alat deskriptif untuk menghitung probabilitas bersyarat. \u200b (Dahulu dibuat Secara Manual) Tingkat kesalahan klasifikasi hanyalah sebagian kecil dari pengamatan pelatihan di suatu wilayah yang bukan milik kelas yang paling umum. $$ E=1-\\max \\left(\\hat{p}_{m k}\\right) $$ \u200b (Formula tingkat kesalahan klasifikasi) Namun, ini tidak cukup sensitif untuk penanaman pohon. Dalam praktiknya, dua metode lain digunakan. $$ G=\\sum_{k=1}^{K} \\hat{p} {m k}\\left(1-\\hat{p} {m k}\\right) $$ \u200b (Gini Index) Ini adalah ukuran varian total di semua kelas. Seperti yang Anda lihat, indeks Gini akan kecil jika proporsinya mendekati 0 atau 1, jadi ini adalah ukuran yang baik untuk pengujian. Hal yang serupa diterapkan pada metode lain yang disebut cross-entropy. $$ D=-\\sum_{k=1}^{K} \\hat{p} {m k} \\log \\left(\\hat{p} {m k}\\right) $$ \u200b (Entropy)","title":"Theory"},{"location":"Decison Trees/#bagging-random-forests-and-boosting","text":"","title":"Bagging, random forests and boosting"},{"location":"Decison Trees/#bagging","text":"\u200b Sebelumnya dapat menghitung standar deviasi dari jumlah yang diinginkan. Untuk pohon keputusan, variansnya sangat tinggi. Oleh karena itu, dengan agregasi bootstrap atau bagging , kita dapat mengurangi varians dan meningkatkan kinerja pohon keputusan. \u200b Bagging melibatkan berulang kali mengambil sampel dari dataset. Ini menghasilkan B set pelatihan bootstrap yang berbeda. Kemudian, melatih pada semua set pelatihan bootstrap untuk mendapatkan prediksi untuk setiap set, dan kemudian rata-rata prediksi. $$ \\hat{f} {b a g}(x)=\\frac{1}{B} \\sum {b=1}^{B} \\hat{f}^{* b}(x) $$ \u200b Menerapkan ini ke pohon keputusan, itu berarti bahwa kita dapat membangun sejumlah besar pohon yang akan memiliki varian tinggi dan bias rendah. Kemudian, kita dapat meratakan prediksi mereka untuk mengurangi varians untuk meningkatkan kinerja pohon keputusan.","title":"Bagging"},{"location":"Decison Trees/#random-forests","text":"\u200b Random Forests memberikan peningkatan dari pada bagging , dengan cara mengecek mulai bagian kecil yang ada pada pohon-pohon tersebut. Seperti dalam bagging, banyak pohon keputusan dibangun. Namun, pada setiap split, sampel acak dari prediktor m dipilih dari semua prediktor p. Pemisahan ini diizinkan untuk menggunakan hanya satu dari prediktor m, Dengan kata lain, pada setiap pemisahan, algoritma tidak diperbolehkan untuk mempertimbangkan mayoritas prediktor yang tersedia! $$ m=\\sqrt{p} $$ ( Kesalahan klasifikasi sebagai fungsi dari jumlah pohon. Setiap baris mewakili jumlah prediktor yang tersedia di setiap pemisahan )","title":"Random forests"},{"location":"Decison Trees/#boosting","text":"\u200b Tidak jauh beda dengan Bagging dan Random forest , namun Bossting mengkoreksi setiap pohon Ini berarti bahwa algoritma belajar dengan lambat. Setiap pohon dengan residu dari model dari pada variabel target. Oleh karena itu, setiap pohon kecil dan perlahan akan meningkatkan prediksi di daerah di mana ia tidak bekerja dengan baik ( Kesalahan klasifikasi sebagai fungsi dari jumlah pohon. Setiap baris mewakili kedalaman interaksi yang berbeda.)","title":"Boosting"},{"location":"Decison Trees/#study-kasus","text":"\u200b Banyak dataset tentang kanker payudara mengandung informasi tentang tumor. Namun, saya beruntung menemukan dataset yang berisi informasi tes darah rutin pasien dengan dan tanpa kanker payudara. Berpotensi, jika kita dapat memprediksi secara akurat jika seorang pasien menderita kanker, pasien tersebut dapat menerima perawatan yang sangat dini, bahkan sebelum tumor terlihat! \u200b Saya memperoleh dataset di sini . Pertama, yang perlu diperhatikan bahwa dataset sangat kecil, dengan hanya 116 instance. Ini menimbulkan beberapa tantangan, karena pohon keputusan mungkin sesuai dengan data, atau model prediksi yang saya lakukan mungkin bukan yang terbaik, karena kurangnya pengamatan lain. Namun, itu adalah bukti konsep yang baik yang mungkin menunjukkan potensi nyata untuk memprediksi kanker payudara dari tes darah sederhana.","title":"Study Kasus"},{"location":"Decison Trees/#step-1-import-package","text":"dengan menggunakan python 3.7 library yang diperlukan : import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns Step 2, Read Data DATAPATH = 'data/dataR2.csv' data = pd.read_csv(DATAPATH) data.head()","title":"Step 1, Import package"},{"location":"Decison Trees/#step-3-classification","text":"Mengklasifikasi data. x = data['Classification'] ax = sns.countplot(x=x, data=data) \u200b (Seperti yang kita lihat, ada jumlah pasien dan kontrol yang hampir sama.)","title":"Step 3, Classification"},{"location":"Decison Trees/#step-4-ploting","text":"Sekarang, akan menarik untuk melihat distribusi dan kepadatan setiap fitur untuk orang sehat dan pasien. Untuk melakukannya, violin plots sangat ideal. Ini menunjukkan kepadatan dan distribusi fitur dalam satu plot. y = data.columns[:-1] x = data.columns[-1] def violin_plots(x, y, data): for i, col in enumerate(y): plt.figure(i) sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.violinplot(x=x, y=col, data=data) violin_plots(x, y, data)","title":"Step 4, Ploting"},{"location":"Decison Trees/#step-5-modelling","text":"Pertama, kita perlu menyandikan kelas ke 0 dan 1: from sklearn.preprocessing import LabelEncoder le = LabelEncoder() data['Classification'] = le.fit_transform(data['Classification']) data.head() Sekarang, 0 mewakili kontrol yang sehat, dan 1 mewakili pasien. Kemudian, membagi dataset menjadi set pelatihan dan tes: from sklearn.model_selection import train_test_split y = data['Classification'].values.reshape(-1, 1) X = data.drop('Classification', 1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","title":"Step 5, Modelling"},{"location":"Decison Trees/#step-6-matrix","text":"Perlu menentukan metrik kesalahan yang sesuai. Karena ini adalah masalah klasifikasi, import itertools def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(\"Normalized confusion matrix\") else: print('Confusion matrix, without normalization') print(cm) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes) plt.yticks(tick_marks, classes) fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\") plt.ylabel('True label') plt.xlabel('Predicted label') plt.tight_layout()","title":"Step 6, Matrix"},{"location":"Decison Trees/#step-7-implementasi","text":"","title":"Step 7, Implementasi"},{"location":"Decison Trees/#decision-tree","text":"from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import confusion_matrix clf = DecisionTreeClassifier() clf.fit(X_train, y_train) y_pred = clf.predict(X_test) decision_tree_cm = confusion_matrix(y_test, y_pred) plot_confusion_matrix(decision_tree_cm, [0, 1]) plt.show()","title":"Decision tree"},{"location":"Decison Trees/#output","text":"Confusion matrix, without normalization [[6 3] [0 3]]","title":"Output"},{"location":"Decison Trees/#bagging_1","text":"from sklearn.ensemble import BaggingClassifier bagging_clf = BaggingClassifier() bagging_clf.fit(X_train, y_train.ravel()) y_pred_bag = bagging_clf.predict(X_test) bag_cm = confusion_matrix(y_test, y_pred_bag) plot_confusion_matrix(bag_cm, [0, 1]) plt.show()","title":"Bagging"},{"location":"Decison Trees/#output_1","text":"Confusion matrix, without normalization [[9 0] [0 3]]","title":"Output"},{"location":"Decison Trees/#random-forest","text":"Di sini, untuk klasifikasi Random forest , Menggunakan 100 dataset: from sklearn.ensemble import RandomForestClassifier random_clf = RandomForestClassifier(100) random_clf.fit(X_train, y_train.ravel()) y_pred_random = random_clf.predict(X_test) random_cm = confusion_matrix(y_test, y_pred_random) plot_confusion_matrix(random_cm, [0, 1]) plt.show()","title":"Random forest"},{"location":"Decison Trees/#output_2","text":"Confusion matrix, without normalization [[9 0] [1 2]]","title":"Output"},{"location":"Decison Trees/#boosting_1","text":"from sklearn.ensemble import GradientBoostingClassifier boost_clf = GradientBoostingClassifier() boost_clf.fit(X_train, y_train.ravel()) y_pred_boost = boost_clf.predict(X_test) boost_cm = confusion_matrix(y_test, y_pred_boost) plot_confusion_matrix(boost_cm, [0, 1]) plt.show()","title":"Boosting"},{"location":"Decison Trees/#output_3","text":"Confusion matrix, without normalization [[8 1] [0 3]]","title":"Output"},{"location":"Decison Trees/#conclusion","text":"\u200b Kita telah melihat bagaimana menerapkan pohon keputusan dan bagaimana meningkatkan kinerjanya dengan bosoting , bagging , dan random forest . Tampaknya bagging memberikan hasil terbaik, karena mengklasifikasikan semua instance dengan benar. Namun, ada yang harus ingat bahwa dataset itu sangat kecil. Meskipun itu menunjukkan bahwa kita berpotensi dapat memprediksi kanker payudara dari tes darah, algoritmanya tidak mungkin bekerja dengan baik pada data yang tidak terlihat, karena tidak ada cukup data.","title":"Conclusion"},{"location":"Decison Trees/#references","text":"Machine Learning Course ( https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd ) Dicision trees in maschine learning ( https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052?source=search_post---------0 ) Deep math machine learning ( https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1 ) Decision trees for machine learning and data science ( https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956 )","title":"References"},{"location":"K Means Clustering/","text":"K Means Clustering \u200b K-means merupakan salah satu algoritma clustering . Tujuan algoritma ini yaitu untuk membagi data menjadi beberapa kelompok. Algoritma ini menerima masukan berupa data tanpa label kelas. Hal ini berbeda dengan supervised learning yang menerima masukan berupa vektor (\u00ad x\u00ad1 , y1 ) , (\u00ad x\u00ad2 , y2 ) , \u2026, (\u00ad x\u00adi , yi ), di mana xi merupakan data dari suatu data pelatihan dan yi merupakan label kelas untuk xi . \u200b Pada algoritma pembelajaran ini, komputer mengelompokkan sendiri data-data yang menjadi masukannya tanpa mengetahui terlebih dulu target kelasnya. Pembelajaran ini termasuk dalam unsupervised learning. Masukan yang diterima adalah data atau objek dan k buah kelompok ( cluster ) yang diinginkan. Algoritma ini akan mengelompokkan data atau objek ke dalam k buah kelompok tersebut. Pada setiap cluster terdapat titik pusat ( centroid ) yang merepresentasikan cluster tersebut. \u200b K-means ditemukan oleh beberapa orang yaitu Lloyd (1957, 1982), Forgey (1965) , Friedman and Rubin (1967) , and McQueen (1967) . Ide dari clustering pertama kali ditemukan oleh Lloyd pada tahun 1957, namun hal tersebut baru dipublikasi pada tahun 1982. Pada tahun 1965, Forgey juga mempublikasi teknik yang sama sehingga terkadang dikenal sebagai Lloyd-Forgy pada beberapa sumber. Kelebihan dan Kekurangan kelebihan pada algoritma k-means Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan pembelajaran ini relatif cepat. Umum digunakan. kekurangan Sebelum algoritma dijalankan, k buah titik diinisialisasi secara random sehingga pengelompokkan data yang dihasilkan dapat berbeda-beda . Jika nilai random untuk inisialisasi kurang baik, maka pengelompokkan yang dihasilkan pun menjadi kurang optimal. Jika hanya terdapat beberapa titik sampel data, maka cukup mudah untuk menghitung dan mencari titik terdekat dengan k titik yang diinisialisasi secara random. Namun jika terdapat banyak sekali titik data (misalnya satu milyar buah data), maka perhitungan dan pencarian titik terdekat akan membutuhkan waktu yang lama. Proses tersebut dapat dipercepat, namun dibutuhkan struktur data yang lebih rumit seperti kD-Tree atau hashing. Wikipedia- Study Kasus Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Mall_Costomer.csv. Step 1, Import package dengan menggunakan python 3.7 library yang diperlukan : import numpy as np import pandas as pd import matplotlib.pyplot as plt Step 2 , Read data dataset=pd.read_csv('Customers.csv') X=dataset.iloc[:,[3,4]].values Step 3, Elbow Method #Using the elbow method to find the optimal number of clusters from sklearn.cluster import KMeans wcss=[] for i in range(1,11): kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11),wcss) plt.title('The Elbow Method') plt.xlabel('Number of Clusters') plt.ylabel('WCSS') plt.show() Step 4, Penyesuaian Data #Fitting K-MEans to the dataset kmeans=KMeans(n_clusters=5,init='k-means++',random_state=0) y_kmeans=kmeans.fit_predict(X) Step 5, Visualisai #Visualize the clusters plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=100,c='red',label='Cluster1') plt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='blue',label='Cluster2') plt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='green',label='Cluster3') plt.scatter(X[y_kmeans==3,0],X[y_kmeans==3,1],s=100,c='cyan',label='Cluster4') plt.scatter(X[y_kmeans==4,0],X[y_kmeans==4,1],s=100,c='magenta',label='Cluster5') plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='yellow',label='Centroids') plt.title('Clusters of customers') plt.xlabel('Annual Income(K$)') plt.ylabel('Spending Score(1-100)') plt.legend() plt.show() Conclusion \u200b Kmeans clustering adalah salah satu algoritma pengelompokan paling populer dan biasanya hal pertama yang diterapkan para praktisi ketika menyelesaikan tugas-tugas pengelompokan untuk mendapatkan gambaran tentang struktur dataset. Tujuan kmeans adalah untuk mengelompokkan titik data ke dalam subkelompok yang tidak tumpang tindih. Itu melakukan pekerjaan yang sangat baik ketika cluster memiliki semacam bentuk bola. Namun, kmean juga tidak mempelajari jumlah cluster dari data dan mengharuskannya untuk ditentukan sebelumnya. Untuk menjadi praktisi yang baik, ada baiknya untuk mengetahui asumsi di balik algoritma / metode sehingga kita akan memiliki ide yang cukup bagus tentang kekuatan dan kelemahan masing-masing metode. References Lecture Slides and Files | Introduction to Computational Thinking and Data Science | Electrical\u2026 (https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-slides-and-files/) K-Means Algorithm - Stanford University | Coursera (https://www.coursera.org/learn/machine-learning/lecture/93VPG/k-means-algorithm) K-mean Clustering Algorithm ( https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a )","title":"K Means Clustering"},{"location":"K Means Clustering/#k-means-clustering","text":"\u200b K-means merupakan salah satu algoritma clustering . Tujuan algoritma ini yaitu untuk membagi data menjadi beberapa kelompok. Algoritma ini menerima masukan berupa data tanpa label kelas. Hal ini berbeda dengan supervised learning yang menerima masukan berupa vektor (\u00ad x\u00ad1 , y1 ) , (\u00ad x\u00ad2 , y2 ) , \u2026, (\u00ad x\u00adi , yi ), di mana xi merupakan data dari suatu data pelatihan dan yi merupakan label kelas untuk xi . \u200b Pada algoritma pembelajaran ini, komputer mengelompokkan sendiri data-data yang menjadi masukannya tanpa mengetahui terlebih dulu target kelasnya. Pembelajaran ini termasuk dalam unsupervised learning. Masukan yang diterima adalah data atau objek dan k buah kelompok ( cluster ) yang diinginkan. Algoritma ini akan mengelompokkan data atau objek ke dalam k buah kelompok tersebut. Pada setiap cluster terdapat titik pusat ( centroid ) yang merepresentasikan cluster tersebut. \u200b K-means ditemukan oleh beberapa orang yaitu Lloyd (1957, 1982), Forgey (1965) , Friedman and Rubin (1967) , and McQueen (1967) . Ide dari clustering pertama kali ditemukan oleh Lloyd pada tahun 1957, namun hal tersebut baru dipublikasi pada tahun 1982. Pada tahun 1965, Forgey juga mempublikasi teknik yang sama sehingga terkadang dikenal sebagai Lloyd-Forgy pada beberapa sumber.","title":"K Means Clustering"},{"location":"K Means Clustering/#kelebihan-dan-kekurangan","text":"kelebihan pada algoritma k-means Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan pembelajaran ini relatif cepat. Umum digunakan. kekurangan Sebelum algoritma dijalankan, k buah titik diinisialisasi secara random sehingga pengelompokkan data yang dihasilkan dapat berbeda-beda . Jika nilai random untuk inisialisasi kurang baik, maka pengelompokkan yang dihasilkan pun menjadi kurang optimal. Jika hanya terdapat beberapa titik sampel data, maka cukup mudah untuk menghitung dan mencari titik terdekat dengan k titik yang diinisialisasi secara random. Namun jika terdapat banyak sekali titik data (misalnya satu milyar buah data), maka perhitungan dan pencarian titik terdekat akan membutuhkan waktu yang lama. Proses tersebut dapat dipercepat, namun dibutuhkan struktur data yang lebih rumit seperti kD-Tree atau hashing. Wikipedia-","title":"Kelebihan dan Kekurangan"},{"location":"K Means Clustering/#study-kasus","text":"Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Mall_Costomer.csv.","title":"Study Kasus"},{"location":"K Means Clustering/#step-1-import-package","text":"dengan menggunakan python 3.7 library yang diperlukan : import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Step 1, Import package"},{"location":"K Means Clustering/#step-2-read-data","text":"dataset=pd.read_csv('Customers.csv') X=dataset.iloc[:,[3,4]].values","title":"Step 2 , Read data"},{"location":"K Means Clustering/#step-3-elbow-method","text":"#Using the elbow method to find the optimal number of clusters from sklearn.cluster import KMeans wcss=[] for i in range(1,11): kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11),wcss) plt.title('The Elbow Method') plt.xlabel('Number of Clusters') plt.ylabel('WCSS') plt.show()","title":"Step 3, Elbow Method"},{"location":"K Means Clustering/#step-4-penyesuaian-data","text":"#Fitting K-MEans to the dataset kmeans=KMeans(n_clusters=5,init='k-means++',random_state=0) y_kmeans=kmeans.fit_predict(X)","title":"Step 4, Penyesuaian Data"},{"location":"K Means Clustering/#step-5-visualisai","text":"#Visualize the clusters plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=100,c='red',label='Cluster1') plt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='blue',label='Cluster2') plt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='green',label='Cluster3') plt.scatter(X[y_kmeans==3,0],X[y_kmeans==3,1],s=100,c='cyan',label='Cluster4') plt.scatter(X[y_kmeans==4,0],X[y_kmeans==4,1],s=100,c='magenta',label='Cluster5') plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='yellow',label='Centroids') plt.title('Clusters of customers') plt.xlabel('Annual Income(K$)') plt.ylabel('Spending Score(1-100)') plt.legend() plt.show()","title":"Step 5, Visualisai"},{"location":"K Means Clustering/#conclusion","text":"\u200b Kmeans clustering adalah salah satu algoritma pengelompokan paling populer dan biasanya hal pertama yang diterapkan para praktisi ketika menyelesaikan tugas-tugas pengelompokan untuk mendapatkan gambaran tentang struktur dataset. Tujuan kmeans adalah untuk mengelompokkan titik data ke dalam subkelompok yang tidak tumpang tindih. Itu melakukan pekerjaan yang sangat baik ketika cluster memiliki semacam bentuk bola. Namun, kmean juga tidak mempelajari jumlah cluster dari data dan mengharuskannya untuk ditentukan sebelumnya. Untuk menjadi praktisi yang baik, ada baiknya untuk mengetahui asumsi di balik algoritma / metode sehingga kita akan memiliki ide yang cukup bagus tentang kekuatan dan kelemahan masing-masing metode.","title":"Conclusion"},{"location":"K Means Clustering/#references","text":"Lecture Slides and Files | Introduction to Computational Thinking and Data Science | Electrical\u2026 (https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-slides-and-files/) K-Means Algorithm - Stanford University | Coursera (https://www.coursera.org/learn/machine-learning/lecture/93VPG/k-means-algorithm) K-mean Clustering Algorithm ( https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a )","title":"References"},{"location":"about/","text":"About Me Name Muhammad Agiel Asy'ari Email agielasyari1@gmail.com Github https://github.com/agielasyari1 Linkedin https://www.linkedin.com/in/agiel-asy-ari-a159b08a/ Instagram https://www.instagram.com/agielasyari1/","title":"About Me"},{"location":"about/#about-me","text":"","title":"About Me"},{"location":"about/#name","text":"Muhammad Agiel Asy'ari","title":"Name"},{"location":"about/#email","text":"agielasyari1@gmail.com","title":"Email"},{"location":"about/#github","text":"https://github.com/agielasyari1","title":"Github"},{"location":"about/#linkedin","text":"https://www.linkedin.com/in/agiel-asy-ari-a159b08a/","title":"Linkedin"},{"location":"about/#instagram","text":"https://www.instagram.com/agielasyari1/","title":"Instagram"}]}