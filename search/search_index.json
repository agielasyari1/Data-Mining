{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KNN or K-Nearest Neighbours Apa Itu KNN (K-Nearest Neighbour) ?? Algoritma K-Nearest Neighbor (KNN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data . Kelebihan dan Kekurangan KNN (K-Nearest Neighbor) Kelebihan KNN (K-Nearest Neighbour) Sangat nonlinear. Mudah dipahami dan diimplementasikan. Kekurangan KNN (K-Nearest Neighbour) Perlu menunjukkan parameter K (jumlah tetangga terdekat). Tidak menangani nilai hilang ( missing value ) secara implisit. Sensitif terhadap data pencilan ( outlier ). Rentan terhadap variabel yang non-informatif. Rentan terhadap dimensionalitas yang tinggi. Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari setiap sampel uji pada keseluruhan sampel latih. Study Kasus Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Social_Network_Ads.csv. Step 1, Import package dengan menggunakan python 3.7 library yang diperlukan : #imporing libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd import time Step 2, Feature scalling Wikipedia- \u201cSince the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\u201d \u200b Jadi karena klasifikasi KNN dihitung berdasarkan jarak Euclidean antar dua titik. Jika salah satu variabel memiliki jarak yang dengan rentang yang jauh(nilai yg jauh lebih tinggi),maka nilai tersebut harus disesuaikan. Jadi jarak antara semua variabel harus dinormalisasikan agar jarak akhir yang didapatkan proposional. #feature scaling class FeatureScaling: def __init__(self,X,y): self.X=X.copy() if y.ndim==1: y=np.reshape(y,(y.shape[0],1)) self.y=y.copy() self.minMax_X={} self.minMax_y={} def fit_transform_X(self): num_of_features=self.X.shape[1] for i in range(num_of_features): feature=self.X[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_X[i]=np.array([Mean,Min,Max]) self.X[:,i]=feature return self.X.copy() def fit_transform_Y(self): num_of_features=self.y.shape[1] for i in range(num_of_features): feature=self.y[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_y[i]=np.array([Mean,Min,Max]) self.y[:,i]=feature return np.reshape(self.y,self.y.shape[0]) def inverse_transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_X[i][1] Max=self.minMax_X[i][2] feature=feature*(Max-Min)+Mean X_transformed[:,i]=feature return X_transformed def inverse_transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=feature*(Max-Min)+Mean y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) X_transformed[:,i]=feature return X_transformed def transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def returnX(self): return self.X def returnY(self): return self.y Step 3 Train \u200b Saya menerapkan kelas KNN dengan fungsi standar 'cocok' untuk pelatihan dan 'prediksi' untuk memprediksi data uji. KNN menggunakan lazy algoritm yang berarti semua perhitungan ditangguhkan hingga prediksi. Dalam metode fit, saya hanya menetapkan data pelatihan ke variabel kelas - xtrain dan ytrain. Tidak diperlukan perhitungan. \u200b Saat saya mengulangi setiap baris pelatihan untuk mendapatkan skor kesamaan, saya menggunakan document_similarity fungsi kustom yang menerima dua teks dan mengembalikan skor kesamaan di antara mereka (0 & 1). Skor kesamaan yang lebih tinggi menunjukkan lebih banyak kesamaan di antara mereka. import numpy as np class KNN: def __init__(self,X_train,Y_train,K): self.X_train=X_train self.Y_train=Y_train self.K=K def predict(self,X): y_pred=np.array([]) for each in X: ed=np.sum((each-self.X_train)**2,axis=1) y_ed=np.concatenate((self.Y_train.reshape(self.Y_train.shape[0],1),ed.reshape(ed.shape[0],1)),axis=1) y_ed=y_ed[y_ed[:,1].argsort()] K_neighbours=y_ed[0:self.K] (values,counts) = np.unique(K_neighbours[:,0].astype(int),return_counts=True) y_pred=np.append(y_pred,values[np.argmax(counts)]) return y_pred Step 4 , Read data #reading dataset Data=pd.read_csv('Social_Network_Ads.csv') print(Data.head(10)) Data.describe() Output User ID Gender Age EstimatedSalary Purchased 0 15624510 Male 19 19000 0 1 15810944 Male 35 20000 0 2 15668575 Female 26 43000 0 3 15603246 Female 27 57000 0 4 15804002 Male 19 76000 0 5 15728773 Male 27 58000 0 6 15598044 Female 27 84000 0 7 15694829 Female 32 150000 1 8 15600575 Male 25 33000 0 9 15727311 Female 35 65000 0 User ID Age EstimatedSalary Purchased count 4.000000e+02 400.000000 400.000000 400.000000 mean 1.569154e+07 37.655000 69742.500000 0.357500 std 7.165832e+04 10.482877 34096.960282 0.479864 min 1.556669e+07 18.000000 15000.000000 0.000000 25% 1.562676e+07 29.750000 43000.000000 0.000000 50% 1.569434e+07 37.000000 70000.000000 0.000000 75% 1.575036e+07 46.000000 88000.000000 1.000000 max 1.581524e+07 60.000000 150000.000000 1.000000 Step 5, Traning & Testing saya menggambil sampel untuk ditraining dari keseluruhan data yaitu 75% dan untuk ditesting 25% #training and testing set size train_size=int(0.75*Data.shape[0]) test_size=int(0.25*Data.shape[0]) print(\"Training set size : \"+ str(train_size)) print(\"Testing set size : \"+str(test_size)) Training set size : 300 Testing set size : 100 #Getting features from dataset Data=Data.sample(frac=1) X=Data.iloc[:,[2, 3]].values y=Data.iloc[:,4].values X=X.astype(float) #feature scaling fs=FeatureScaling(X,y) X=fs.fit_transform_X() #training set split X_train=X[0:train_size,:] Y_train=y[0:train_size] #testing set split X_test=X[train_size:,:] Y_test=y[train_size:] l=time.time() knn=KNN(X_train,Y_train,5) y_pred=knn.predict(X_test) r=time.time() KNN_learn_time=(r-l) print(r-l) 0.017045021057128906 Step 6, Getting Matrix #getting the confusion matrix tp=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==0 and y_pred[i]==0]) tn=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==0 and y_pred[i]==1]) fp=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==1 and y_pred[i]==0]) fn=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==1 and y_pred[i]==1]) confusion_matrix=np.array([[tp,tn],[fp,fn]]) print(confusion_matrix) [[54 8] [ 6 32]] Step 7, Getting time Sebagai perbandingan waktu saja #Same algorithm using sklearn KNN just for comparsion purpose from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) l=time.time() classifier.fit(X_train, Y_train) y_pred_sklearn = classifier.predict(X_test) r=time.time() sklearn_time=(r-l) print(sklearn_time) 0.0020058155059814453 print(\"But sklearn time is faster than our implementation by: \"+str(KNN_learn_time/sklearn_time)+\" times\") But sklearn time is faster than our implementation by: 8.497801022227504 times # Making the Confusion Matrix from sklearn.metrics import confusion_matrix cm = confusion_matrix(Y_test, y_pred_sklearn) print(cm) [[54 8] [ 6 32]] Step 8, Implementasi matplotlib # Visualising the Training set results for our implementation l=time.time() from matplotlib.colors import ListedColormap X_set, y_set = X_train, Y_train X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Training set) using our implementation') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 25.802637815475464 seconds # Visualising the Test set results for our implementation l=time.time() X_set, y_set = X_test, Y_test X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Test set) using our implementation') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 22.427660942077637 seconds Skalearn # Visualising the Training set results for sklearn class l=time.time() X_set, y_set = X_train, Y_train X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Training set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 0.48911190032958984 seconds # Visualising the Test set results for sklearn class l=time.time() X_set, y_set = X_test, Y_test X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Test set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 0.47005295753479004 seconds Kesimpulan Algoritma KNN sangat intuitif dan mudah dimengerti, Waktu pengujian bisa sangat lama, karena algoritme melingkar di seluruh dataset training dan menghitung jarak (perhitungan jarak dapat menjadi lambat, berdasarkan pada jenis matrik jarak dan berdasarkan pada jenis dataset), Kumpulan data harus numerik atau matrik jarak harus ada untuk menghitung jarak antar titik, Tidak terlalu baik untuk digunakan pada data yang tidak seimbang References Advernesia. (2018, Mei 28). Pengertian dan Cara Kerja Algoritma K-Nearest Neighbors (KNN) . Retrieved from https://www.advernesia.com/blog/data-science/pengertian-dan-cara-kerja-algoritma-k-nearest-neighbours-knn/ Ismail, A. M. (2018, Agustus 17). Cara Kerja Algoritma k-Nearest Neighbor (k-NN) . Retrieved from https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e Solahudin, Y. (2017, Juli 1). Konsep Package dan Module di Python . Retrieved from https://medium.com/@yanwarsolah/konsep-package-dan-module-di-python-fe3e89e80d40 Zakka, K. (2016, Juli 13). Panduan Lengkap untuk K-Nearest-Neighbors dengan Aplikasi dalam Python dan R . Retrieved from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/","title":"KNN or K-Nearest Neighbours"},{"location":"#knn-or-k-nearest-neighbours","text":"","title":"KNN or K-Nearest Neighbours"},{"location":"#apa-itu-knn-k-nearest-neighbour","text":"Algoritma K-Nearest Neighbor (KNN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data .","title":"Apa Itu KNN (K-Nearest Neighbour) ??"},{"location":"#kelebihan-dan-kekurangan-knn-k-nearest-neighbor","text":"Kelebihan KNN (K-Nearest Neighbour) Sangat nonlinear. Mudah dipahami dan diimplementasikan. Kekurangan KNN (K-Nearest Neighbour) Perlu menunjukkan parameter K (jumlah tetangga terdekat). Tidak menangani nilai hilang ( missing value ) secara implisit. Sensitif terhadap data pencilan ( outlier ). Rentan terhadap variabel yang non-informatif. Rentan terhadap dimensionalitas yang tinggi. Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari setiap sampel uji pada keseluruhan sampel latih.","title":"Kelebihan dan Kekurangan KNN (K-Nearest Neighbor)"},{"location":"#study-kasus","text":"Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Social_Network_Ads.csv.","title":"Study Kasus"},{"location":"#step-1-import-package","text":"dengan menggunakan python 3.7 library yang diperlukan : #imporing libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd import time","title":"Step 1, Import package"},{"location":"#step-2-feature-scalling","text":"Wikipedia- \u201cSince the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\u201d \u200b Jadi karena klasifikasi KNN dihitung berdasarkan jarak Euclidean antar dua titik. Jika salah satu variabel memiliki jarak yang dengan rentang yang jauh(nilai yg jauh lebih tinggi),maka nilai tersebut harus disesuaikan. Jadi jarak antara semua variabel harus dinormalisasikan agar jarak akhir yang didapatkan proposional. #feature scaling class FeatureScaling: def __init__(self,X,y): self.X=X.copy() if y.ndim==1: y=np.reshape(y,(y.shape[0],1)) self.y=y.copy() self.minMax_X={} self.minMax_y={} def fit_transform_X(self): num_of_features=self.X.shape[1] for i in range(num_of_features): feature=self.X[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_X[i]=np.array([Mean,Min,Max]) self.X[:,i]=feature return self.X.copy() def fit_transform_Y(self): num_of_features=self.y.shape[1] for i in range(num_of_features): feature=self.y[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_y[i]=np.array([Mean,Min,Max]) self.y[:,i]=feature return np.reshape(self.y,self.y.shape[0]) def inverse_transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_X[i][1] Max=self.minMax_X[i][2] feature=feature*(Max-Min)+Mean X_transformed[:,i]=feature return X_transformed def inverse_transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=feature*(Max-Min)+Mean y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) X_transformed[:,i]=feature return X_transformed def transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def returnX(self): return self.X def returnY(self): return self.y","title":"Step 2, Feature scalling"},{"location":"#step-3-train","text":"\u200b Saya menerapkan kelas KNN dengan fungsi standar 'cocok' untuk pelatihan dan 'prediksi' untuk memprediksi data uji. KNN menggunakan lazy algoritm yang berarti semua perhitungan ditangguhkan hingga prediksi. Dalam metode fit, saya hanya menetapkan data pelatihan ke variabel kelas - xtrain dan ytrain. Tidak diperlukan perhitungan. \u200b Saat saya mengulangi setiap baris pelatihan untuk mendapatkan skor kesamaan, saya menggunakan document_similarity fungsi kustom yang menerima dua teks dan mengembalikan skor kesamaan di antara mereka (0 & 1). Skor kesamaan yang lebih tinggi menunjukkan lebih banyak kesamaan di antara mereka. import numpy as np class KNN: def __init__(self,X_train,Y_train,K): self.X_train=X_train self.Y_train=Y_train self.K=K def predict(self,X): y_pred=np.array([]) for each in X: ed=np.sum((each-self.X_train)**2,axis=1) y_ed=np.concatenate((self.Y_train.reshape(self.Y_train.shape[0],1),ed.reshape(ed.shape[0],1)),axis=1) y_ed=y_ed[y_ed[:,1].argsort()] K_neighbours=y_ed[0:self.K] (values,counts) = np.unique(K_neighbours[:,0].astype(int),return_counts=True) y_pred=np.append(y_pred,values[np.argmax(counts)]) return y_pred","title":"Step 3 Train"},{"location":"#step-4-read-data","text":"#reading dataset Data=pd.read_csv('Social_Network_Ads.csv') print(Data.head(10)) Data.describe()","title":"Step 4 , Read data"},{"location":"#output","text":"User ID Gender Age EstimatedSalary Purchased 0 15624510 Male 19 19000 0 1 15810944 Male 35 20000 0 2 15668575 Female 26 43000 0 3 15603246 Female 27 57000 0 4 15804002 Male 19 76000 0 5 15728773 Male 27 58000 0 6 15598044 Female 27 84000 0 7 15694829 Female 32 150000 1 8 15600575 Male 25 33000 0 9 15727311 Female 35 65000 0 User ID Age EstimatedSalary Purchased count 4.000000e+02 400.000000 400.000000 400.000000 mean 1.569154e+07 37.655000 69742.500000 0.357500 std 7.165832e+04 10.482877 34096.960282 0.479864 min 1.556669e+07 18.000000 15000.000000 0.000000 25% 1.562676e+07 29.750000 43000.000000 0.000000 50% 1.569434e+07 37.000000 70000.000000 0.000000 75% 1.575036e+07 46.000000 88000.000000 1.000000 max 1.581524e+07 60.000000 150000.000000 1.000000","title":"Output"},{"location":"#step-5-traning-testing","text":"saya menggambil sampel untuk ditraining dari keseluruhan data yaitu 75% dan untuk ditesting 25% #training and testing set size train_size=int(0.75*Data.shape[0]) test_size=int(0.25*Data.shape[0]) print(\"Training set size : \"+ str(train_size)) print(\"Testing set size : \"+str(test_size)) Training set size : 300 Testing set size : 100 #Getting features from dataset Data=Data.sample(frac=1) X=Data.iloc[:,[2, 3]].values y=Data.iloc[:,4].values X=X.astype(float) #feature scaling fs=FeatureScaling(X,y) X=fs.fit_transform_X() #training set split X_train=X[0:train_size,:] Y_train=y[0:train_size] #testing set split X_test=X[train_size:,:] Y_test=y[train_size:] l=time.time() knn=KNN(X_train,Y_train,5) y_pred=knn.predict(X_test) r=time.time() KNN_learn_time=(r-l) print(r-l) 0.017045021057128906","title":"Step 5, Traning &amp; Testing"},{"location":"#step-6-getting-matrix","text":"#getting the confusion matrix tp=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==0 and y_pred[i]==0]) tn=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==0 and y_pred[i]==1]) fp=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==1 and y_pred[i]==0]) fn=len([i for i in range(0,Y_test.shape[0]) if Y_test[i]==1 and y_pred[i]==1]) confusion_matrix=np.array([[tp,tn],[fp,fn]]) print(confusion_matrix) [[54 8] [ 6 32]]","title":"Step 6, Getting Matrix"},{"location":"#step-7-getting-time","text":"Sebagai perbandingan waktu saja #Same algorithm using sklearn KNN just for comparsion purpose from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) l=time.time() classifier.fit(X_train, Y_train) y_pred_sklearn = classifier.predict(X_test) r=time.time() sklearn_time=(r-l) print(sklearn_time) 0.0020058155059814453 print(\"But sklearn time is faster than our implementation by: \"+str(KNN_learn_time/sklearn_time)+\" times\") But sklearn time is faster than our implementation by: 8.497801022227504 times # Making the Confusion Matrix from sklearn.metrics import confusion_matrix cm = confusion_matrix(Y_test, y_pred_sklearn) print(cm) [[54 8] [ 6 32]]","title":"Step 7, Getting time"},{"location":"#step-8-implementasi","text":"","title":"Step 8, Implementasi"},{"location":"#matplotlib","text":"# Visualising the Training set results for our implementation l=time.time() from matplotlib.colors import ListedColormap X_set, y_set = X_train, Y_train X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Training set) using our implementation') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 25.802637815475464 seconds # Visualising the Test set results for our implementation l=time.time() X_set, y_set = X_test, Y_test X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Test set) using our implementation') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 22.427660942077637 seconds","title":"matplotlib"},{"location":"#skalearn","text":"# Visualising the Training set results for sklearn class l=time.time() X_set, y_set = X_train, Y_train X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Training set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 0.48911190032958984 seconds # Visualising the Test set results for sklearn class l=time.time() X_set, y_set = X_test, Y_test X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('orange', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j,marker='.') plt.title('K-NN (Test set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() r=time.time() print(\"Time required for plotting is: \"+str(r-l)+\" seconds\") Time required for plotting is: 0.47005295753479004 seconds","title":"Skalearn"},{"location":"#kesimpulan","text":"Algoritma KNN sangat intuitif dan mudah dimengerti, Waktu pengujian bisa sangat lama, karena algoritme melingkar di seluruh dataset training dan menghitung jarak (perhitungan jarak dapat menjadi lambat, berdasarkan pada jenis matrik jarak dan berdasarkan pada jenis dataset), Kumpulan data harus numerik atau matrik jarak harus ada untuk menghitung jarak antar titik, Tidak terlalu baik untuk digunakan pada data yang tidak seimbang","title":"Kesimpulan"},{"location":"#references","text":"Advernesia. (2018, Mei 28). Pengertian dan Cara Kerja Algoritma K-Nearest Neighbors (KNN) . Retrieved from https://www.advernesia.com/blog/data-science/pengertian-dan-cara-kerja-algoritma-k-nearest-neighbours-knn/ Ismail, A. M. (2018, Agustus 17). Cara Kerja Algoritma k-Nearest Neighbor (k-NN) . Retrieved from https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e Solahudin, Y. (2017, Juli 1). Konsep Package dan Module di Python . Retrieved from https://medium.com/@yanwarsolah/konsep-package-dan-module-di-python-fe3e89e80d40 Zakka, K. (2016, Juli 13). Panduan Lengkap untuk K-Nearest-Neighbors dengan Aplikasi dalam Python dan R . Retrieved from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/","title":"References"},{"location":"K Means Clustering/","text":"K Means Clustering \u200b K-means merupakan salah satu algoritma clustering . Tujuan algoritma ini yaitu untuk membagi data menjadi beberapa kelompok. Algoritma ini menerima masukan berupa data tanpa label kelas. Hal ini berbeda dengan supervised learning yang menerima masukan berupa vektor (\u00ad x\u00ad1 , y1 ) , (\u00ad x\u00ad2 , y2 ) , \u2026, (\u00ad x\u00adi , yi ), di mana xi merupakan data dari suatu data pelatihan dan yi merupakan label kelas untuk xi . \u200b Pada algoritma pembelajaran ini, komputer mengelompokkan sendiri data-data yang menjadi masukannya tanpa mengetahui terlebih dulu target kelasnya. Pembelajaran ini termasuk dalam unsupervised learning. Masukan yang diterima adalah data atau objek dan k buah kelompok ( cluster ) yang diinginkan. Algoritma ini akan mengelompokkan data atau objek ke dalam k buah kelompok tersebut. Pada setiap cluster terdapat titik pusat ( centroid ) yang merepresentasikan cluster tersebut. \u200b K-means ditemukan oleh beberapa orang yaitu Lloyd (1957, 1982), Forgey (1965) , Friedman and Rubin (1967) , and McQueen (1967) . Ide dari clustering pertama kali ditemukan oleh Lloyd pada tahun 1957, namun hal tersebut baru dipublikasi pada tahun 1982. Pada tahun 1965, Forgey juga mempublikasi teknik yang sama sehingga terkadang dikenal sebagai Lloyd-Forgy pada beberapa sumber. Kelebihan dan Kekurangan kelebihan pada algoritma k-means Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan pembelajaran ini relatif cepat. Umum digunakan. kekurangan Sebelum algoritma dijalankan, k buah titik diinisialisasi secara random sehingga pengelompokkan data yang dihasilkan dapat berbeda-beda . Jika nilai random untuk inisialisasi kurang baik, maka pengelompokkan yang dihasilkan pun menjadi kurang optimal. Jika hanya terdapat beberapa titik sampel data, maka cukup mudah untuk menghitung dan mencari titik terdekat dengan k titik yang diinisialisasi secara random. Namun jika terdapat banyak sekali titik data (misalnya satu milyar buah data), maka perhitungan dan pencarian titik terdekat akan membutuhkan waktu yang lama. Proses tersebut dapat dipercepat, namun dibutuhkan struktur data yang lebih rumit seperti kD-Tree atau hashing. Wikipedia- Study Kasus Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Mall_Costomer.csv. Step 1, Import package dengan menggunakan python 3.7 library yang diperlukan : import numpy as np import pandas as pd import matplotlib.pyplot as plt Step 2 , Read data dataset=pd.read_csv('Customers.csv') X=dataset.iloc[:,[3,4]].values Step 3, Elbow Method #Using the elbow method to find the optimal number of clusters from sklearn.cluster import KMeans wcss=[] for i in range(1,11): kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11),wcss) plt.title('The Elbow Method') plt.xlabel('Number of Clusters') plt.ylabel('WCSS') plt.show() Step 4, Penyesuaian Data #Fitting K-MEans to the dataset kmeans=KMeans(n_clusters=5,init='k-means++',random_state=0) y_kmeans=kmeans.fit_predict(X) Step 5, Visualisai #Visualize the clusters plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=100,c='red',label='Cluster1') plt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='blue',label='Cluster2') plt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='green',label='Cluster3') plt.scatter(X[y_kmeans==3,0],X[y_kmeans==3,1],s=100,c='cyan',label='Cluster4') plt.scatter(X[y_kmeans==4,0],X[y_kmeans==4,1],s=100,c='magenta',label='Cluster5') plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='yellow',label='Centroids') plt.title('Clusters of customers') plt.xlabel('Annual Income(K$)') plt.ylabel('Spending Score(1-100)') plt.legend() plt.show() Kesimpulan Kmeans clustering adalah salah satu algoritma pengelompokan paling populer dan biasanya hal pertama yang diterapkan para praktisi ketika menyelesaikan tugas-tugas pengelompokan untuk mendapatkan gambaran tentang struktur dataset. Tujuan kmeans adalah untuk mengelompokkan titik data ke dalam subkelompok yang tidak tumpang tindih. Itu melakukan pekerjaan yang sangat baik ketika cluster memiliki semacam bentuk bola. Namun, ia menderita karena bentuk-bentuk geometri klaster menyimpang dari bentuk-bentuk bola. Selain itu, ia juga tidak mempelajari jumlah cluster dari data dan mengharuskannya untuk ditentukan sebelumnya. Untuk menjadi praktisi yang baik, ada baiknya untuk mengetahui asumsi di balik algoritma / metode sehingga Anda akan memiliki ide yang cukup bagus tentang kekuatan dan kelemahan masing-masing metode. Refences","title":"K Means Clustering"},{"location":"K Means Clustering/#k-means-clustering","text":"\u200b K-means merupakan salah satu algoritma clustering . Tujuan algoritma ini yaitu untuk membagi data menjadi beberapa kelompok. Algoritma ini menerima masukan berupa data tanpa label kelas. Hal ini berbeda dengan supervised learning yang menerima masukan berupa vektor (\u00ad x\u00ad1 , y1 ) , (\u00ad x\u00ad2 , y2 ) , \u2026, (\u00ad x\u00adi , yi ), di mana xi merupakan data dari suatu data pelatihan dan yi merupakan label kelas untuk xi . \u200b Pada algoritma pembelajaran ini, komputer mengelompokkan sendiri data-data yang menjadi masukannya tanpa mengetahui terlebih dulu target kelasnya. Pembelajaran ini termasuk dalam unsupervised learning. Masukan yang diterima adalah data atau objek dan k buah kelompok ( cluster ) yang diinginkan. Algoritma ini akan mengelompokkan data atau objek ke dalam k buah kelompok tersebut. Pada setiap cluster terdapat titik pusat ( centroid ) yang merepresentasikan cluster tersebut. \u200b K-means ditemukan oleh beberapa orang yaitu Lloyd (1957, 1982), Forgey (1965) , Friedman and Rubin (1967) , and McQueen (1967) . Ide dari clustering pertama kali ditemukan oleh Lloyd pada tahun 1957, namun hal tersebut baru dipublikasi pada tahun 1982. Pada tahun 1965, Forgey juga mempublikasi teknik yang sama sehingga terkadang dikenal sebagai Lloyd-Forgy pada beberapa sumber.","title":"K Means Clustering"},{"location":"K Means Clustering/#kelebihan-dan-kekurangan","text":"kelebihan pada algoritma k-means Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan pembelajaran ini relatif cepat. Umum digunakan. kekurangan Sebelum algoritma dijalankan, k buah titik diinisialisasi secara random sehingga pengelompokkan data yang dihasilkan dapat berbeda-beda . Jika nilai random untuk inisialisasi kurang baik, maka pengelompokkan yang dihasilkan pun menjadi kurang optimal. Jika hanya terdapat beberapa titik sampel data, maka cukup mudah untuk menghitung dan mencari titik terdekat dengan k titik yang diinisialisasi secara random. Namun jika terdapat banyak sekali titik data (misalnya satu milyar buah data), maka perhitungan dan pencarian titik terdekat akan membutuhkan waktu yang lama. Proses tersebut dapat dipercepat, namun dibutuhkan struktur data yang lebih rumit seperti kD-Tree atau hashing. Wikipedia-","title":"Kelebihan dan Kekurangan"},{"location":"K Means Clustering/#study-kasus","text":"Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Mall_Costomer.csv.","title":"Study Kasus"},{"location":"K Means Clustering/#step-1-import-package","text":"dengan menggunakan python 3.7 library yang diperlukan : import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Step 1, Import package"},{"location":"K Means Clustering/#step-2-read-data","text":"dataset=pd.read_csv('Customers.csv') X=dataset.iloc[:,[3,4]].values","title":"Step 2 , Read data"},{"location":"K Means Clustering/#step-3-elbow-method","text":"#Using the elbow method to find the optimal number of clusters from sklearn.cluster import KMeans wcss=[] for i in range(1,11): kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11),wcss) plt.title('The Elbow Method') plt.xlabel('Number of Clusters') plt.ylabel('WCSS') plt.show()","title":"Step 3, Elbow Method"},{"location":"K Means Clustering/#step-4-penyesuaian-data","text":"#Fitting K-MEans to the dataset kmeans=KMeans(n_clusters=5,init='k-means++',random_state=0) y_kmeans=kmeans.fit_predict(X)","title":"Step 4, Penyesuaian Data"},{"location":"K Means Clustering/#step-5-visualisai","text":"#Visualize the clusters plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=100,c='red',label='Cluster1') plt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='blue',label='Cluster2') plt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='green',label='Cluster3') plt.scatter(X[y_kmeans==3,0],X[y_kmeans==3,1],s=100,c='cyan',label='Cluster4') plt.scatter(X[y_kmeans==4,0],X[y_kmeans==4,1],s=100,c='magenta',label='Cluster5') plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='yellow',label='Centroids') plt.title('Clusters of customers') plt.xlabel('Annual Income(K$)') plt.ylabel('Spending Score(1-100)') plt.legend() plt.show()","title":"Step 5, Visualisai"},{"location":"K Means Clustering/#kesimpulan","text":"Kmeans clustering adalah salah satu algoritma pengelompokan paling populer dan biasanya hal pertama yang diterapkan para praktisi ketika menyelesaikan tugas-tugas pengelompokan untuk mendapatkan gambaran tentang struktur dataset. Tujuan kmeans adalah untuk mengelompokkan titik data ke dalam subkelompok yang tidak tumpang tindih. Itu melakukan pekerjaan yang sangat baik ketika cluster memiliki semacam bentuk bola. Namun, ia menderita karena bentuk-bentuk geometri klaster menyimpang dari bentuk-bentuk bola. Selain itu, ia juga tidak mempelajari jumlah cluster dari data dan mengharuskannya untuk ditentukan sebelumnya. Untuk menjadi praktisi yang baik, ada baiknya untuk mengetahui asumsi di balik algoritma / metode sehingga Anda akan memiliki ide yang cukup bagus tentang kekuatan dan kelemahan masing-masing metode.","title":"Kesimpulan"},{"location":"K Means Clustering/#refences","text":"","title":"Refences"},{"location":"about/","text":"About Me Name Muhammad Agiel Asy'ari Email agielasyari1@gmail.com Github https://github.com/agielasyari1 Linkedin https://www.linkedin.com/in/agiel-asy-ari-a159b08a/ Instagram https://www.instagram.com/agielasyari1/","title":"About Me"},{"location":"about/#about-me","text":"","title":"About Me"},{"location":"about/#name","text":"Muhammad Agiel Asy'ari","title":"Name"},{"location":"about/#email","text":"agielasyari1@gmail.com","title":"Email"},{"location":"about/#github","text":"https://github.com/agielasyari1","title":"Github"},{"location":"about/#linkedin","text":"https://www.linkedin.com/in/agiel-asy-ari-a159b08a/","title":"Linkedin"},{"location":"about/#instagram","text":"https://www.instagram.com/agielasyari1/","title":"Instagram"}]}